# Bert-clef-encoding

Short code to show the amount of information lost while using BERT encoding, with 'bert-base-uncoded':

| Statistic | Value |
|-----------|-------|
| Average percentage of words lost | 23.18% |
| Average number of words lost | 66.94 |
| Maximum percentage of words lost | 87.50% |

Note to run, you need to download [all_clean.jsonl](https://drive.google.com/file/d/1kppExc6Wo81sCPYI2hkSsxO-ekgD2Qcc/view?pli=1) from this study [^1]

[^1]: Mao, X., Koopman, B., & Zuccon, G. (2024). A Reproducibility Study of Goldilocks: Just-Right Tuning of BERT for TAR. In European Conference on Information Retrieval (ECIR '24) (pp. 132-146). Springer. https://doi.org/10.1007/978-3-031-56066-8_13